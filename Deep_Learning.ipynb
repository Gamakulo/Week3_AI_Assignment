{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77350170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Deep Learning with TensorFlow/PyTorch (MNIST Dataset)\n",
    "#Step 1: Library loading\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Step 2: Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1] and reshape for CNN (add channel dimension)\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1) # Shape becomes (60000, 28, 28, 1)\n",
    "x_test = np.expand_dims(x_test, -1)   # Shape becomes (10000, 28, 28, 1)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Step 3: Build the CNN model architecture\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5), # Regularization to prevent overfitting\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
    "# Save the model\n",
    "model.save('my_mnist_model.h5')\n",
    "print(\"Model saved as 'my_mnist_model.h5'\")\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test loss: {score[0]:.4f}\")\n",
    "print(f\"Test accuracy: {score[1]:.4f}\")\n",
    "\n",
    "# Step 7: Visualize training history and sample predictions\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('mnist_training_history.png') \n",
    "plt.show()\n",
    "\n",
    "# Predict on 5 sample images\n",
    "sample_images = x_test[:5]\n",
    "predictions = model.predict(sample_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display the samples and their predictions\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(sample_images[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Pred: {predicted_classes[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.savefig('mnist_sample_predictions.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
